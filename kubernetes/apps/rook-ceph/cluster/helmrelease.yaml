---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/ocirepository-source-v1beta2.json
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: OCIRepository
metadata:
  name: rook-ceph-cluster
spec:
  interval: 1h
  layerSelector:
    mediaType: application/vnd.cncf.helm.chart.content.v1.tar+gzip
    operation: copy
  ref:
    tag: v1.16.6
  url: oci://ghcr.io/rook/rook-ceph-cluster
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: rook-ceph-cluster
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: rook-ceph
      namespace: rook-ceph
  values:
    # https://rook.io/docs/rook/latest-release/Helm-Charts/ceph-cluster-chart/#configuration

    monitoring:
      enabled: true
      createPrometheusRules: true

    cephClusterSpec:
      cephConfig:
        global:
          bdev_enable_discard: "true"
          bdev_async_discard_threads: "1"
          device_failure_prediction_mode: "local"
          osd_class_update_on_start: "false"

      csi:
        readAffinity:
          enabled: true

      mgr:
        modules:
          - name: diskprediction_local
            enabled: true
          - name: insights
            enabled: true
          - name: pg_autoscaler
            enabled: true
          - name: rook
            enabled: true

      dashboard:
        enabled: true
        ssl: false
        prometheusEndpoint: http://prometheus-operated.observability.svc.cluster.local:9090

      network:
        provider: host
        connections:
          requireMsgr2: true

      storage:
        useAllNodes: false
        useAllDevices: false

        config:
          encryptedDevice: "true"
          osdsPerDevice: "1"

        nodes:
          - name: talos-bd795i-01
            devices:
              - name: /dev/disk/by-id/nvme-Samsung_SSD_980_PRO_2TB_S6B0NU0W405703D
          - name: talos-prox-01
            devices:
              - name: /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1
          - name: talos-prox-02
            devices:
              - name: /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_drive-scsi1

    cephBlockPools:
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3

        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: ["discard"]

          parameters:
            # As much as I'd like to use zstd, the Ceph docs say that it is "*not* recommended for
            # BlueStore due to high CPU overhead when compressing small amounts of data"
            # https://docs.ceph.com/en/reef/rados/configuration/bluestore-config-ref/#confval-bluestore_compression_algorithm
            compression_algorithm: "lz4"
            compression_mode: "aggressive"

            # RBD image format
            imageFormat: "2"

            # RBD image features. Linux kernel 5.4+ supports all features
            imageFeatures: layering,exclusive-lock,object-map,fast-diff,deep-flatten

            # Secrets automatically created by Rook
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

            # Volume filesystem
            csi.storage.k8s.io/fstype: ext4

    cephBlockPoolsVolumeSnapshotClass:
      enabled: true
      name: ceph-block
      isDefault: false
      deletionPolicy: Delete

    cephFileSystems: []
    cephObjectStores: []
